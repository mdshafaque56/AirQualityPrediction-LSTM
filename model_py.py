# -*- coding: utf-8 -*-
"""Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G95scQNpW252yqNMkSz0bWOqrRHf4QDf
"""

import pandas as pd
import os

df_ozone = pd.read_csv("/content/O3_Dataset.csv")
df_co = pd.read_csv("/content/CO_Dataset.csv")

print(df_ozone.head())
print(df_co.head())

df_ozone.keys()

df_co.keys()

# Keep only relevant columns (assuming 'Date', 'Time', 'Ozone', 'CO' exist)
df_ozone = df_ozone[ [
    'Date Local', 'Time Local', 'Latitude', 'Longitude', 'Mean Including All Data'
]]
df_co = df_co[ [
    'Date Local', 'Time Local', 'Latitude', 'Longitude', 'Mean Including All Data'
]]

import pandas as pd

# Function to process both datasets
def preprocess_air_quality(df):
    # Rename target column for clarity
    df.rename(columns={'Mean Including All Data': 'Pollutant_Level'}, inplace=True)

    # Combine 'Date Local' and 'Time Local' into a single DateTime column
    df['Datetime'] = pd.to_datetime(df['Date Local'] + ' ' + df['Time Local'], errors='coerce')

    # Drop the original date and time columns
    df.drop(columns=['Date Local', 'Time Local'], inplace=True)

    # Sort data by datetime (important for time-series analysis)
    df = df.sort_values(by='Datetime').reset_index(drop=True)

    return df

# Apply preprocessing to both datasets
df_ozone = preprocess_air_quality(df_ozone)
df_co = preprocess_air_quality(df_co)

# Display first few rows
print(df_ozone.head())
print(df_co.head())

# Fill missing pollutant values with column mean
df_ozone['Pollutant_Level'].fillna(df_ozone['Pollutant_Level'].mean())
df_co['Pollutant_Level'].fillna(df_co['Pollutant_Level'].mean())

print(df_ozone.head())
print(df_co.head())

import matplotlib.pyplot as plt
import seaborn as sns

# Ozone dataset geographical distribution
plt.figure(figsize=(12, 6))
plt.scatter(df_ozone['Longitude'], df_ozone['Latitude'], c=df_ozone['Pollutant_Level'], cmap='viridis', alpha=0.7)
plt.colorbar(label='Ozone Level (ppm)')
plt.title('Geographical Distribution of Ozone Monitoring Sites')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.grid(True)
plt.tight_layout()
plt.show()

# CO dataset geographical distribution
plt.figure(figsize=(12, 6))
plt.scatter(df_co['Longitude'], df_co['Latitude'], c=df_co['Pollutant_Level'], cmap='plasma', alpha=0.7)
plt.colorbar(label='CO Level (ppm)')
plt.title('Geographical Distribution of CO Monitoring Sites')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.preprocessing import MinMaxScaler

scaler_ozone = MinMaxScaler()
scaler_co = MinMaxScaler()

# Scale pollutant levels
df_ozone['Pollutant_Level'] = scaler_ozone.fit_transform(df_ozone[['Pollutant_Level']])
df_co['Pollutant_Level'] = scaler_co.fit_transform(df_co[['Pollutant_Level']])

# Scale latitude and longitude (optional for spatial analysis)
df_ozone[['Latitude', 'Longitude']] = scaler_ozone.fit_transform(df_ozone[['Latitude', 'Longitude']])
df_co[['Latitude', 'Longitude']] = scaler_co.fit_transform(df_co[['Latitude', 'Longitude']])

print(df_ozone.head())
print(df_co.head())

import numpy as np

def create_sequences(data, sequence_length=24):
    X, y = [], []
    for i in range(len(data) - sequence_length):
        X.append(data[i:i+sequence_length])
        y.append(data[i+sequence_length])
    return np.array(X), np.array(y)

# Convert to NumPy arrays (only using 'Pollutant_Level' for prediction)
ozone_values = df_ozone[['Pollutant_Level']].values
co_values = df_co[['Pollutant_Level']].values

# Create sequences
sequence_length = 24  # Use the last 24 hours to predict the next hour
X_ozone, y_ozone = create_sequences(ozone_values, sequence_length)
X_co, y_co = create_sequences(co_values, sequence_length)

print("Shape of X_ozone:", X_ozone.shape)  # (samples, sequence_length, features)
print("Shape of y_ozone:", y_ozone.shape)  # (samples, 1)
print("Shape of X_co:", X_co.shape)
print("Shape of y_co:", y_co.shape)

sampling_rate = 100  # Keep every 100th row

X_ozone_sampled = X_ozone[::sampling_rate]
y_ozone_sampled = y_ozone[::sampling_rate]

X_co_sampled = X_co[::sampling_rate]
y_co_sampled = y_co[::sampling_rate]

print("Reduced Shape of X_ozone:", X_ozone_sampled.shape)
print("Reduced Shape of y_ozone:", y_ozone_sampled.shape)
print("Reduced Shape of X_co:", X_co_sampled.shape)
print("Reduced Shape of y_co:", y_co_sampled.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# LSTM Model
def build_lstm_model():
    model = Sequential([
        LSTM(64, return_sequences=True, input_shape=(24, 1)),  # 24-hour sequence
        Dropout(0.2),
        LSTM(32, return_sequences=False),
        Dropout(0.2),
        Dense(16, activation='relu'),
        Dense(1)  # Predicting pollutant level
    ])

    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    return model

# Create separate models for Ozone and CO
model_ozone = build_lstm_model()
model_co = build_lstm_model()

model_ozone.summary()
model_co.summary()

# Train Ozone Model
history_ozone = model_ozone.fit(
    X_ozone_sampled, y_ozone_sampled,
    epochs=50, batch_size=64, validation_split=0.2
)

# Train CO Model
history_co = model_co.fit(
    X_co_sampled, y_co_sampled,
    epochs=50, batch_size=64, validation_split=0.2
)

from sklearn.model_selection import train_test_split

# For ozone
X_train_oz, X_test_oz, y_train_oz, y_test_oz = train_test_split(
    X_ozone_sampled, y_ozone_sampled, test_size=0.2, shuffle=False
)

# For CO
X_train_co, X_test_co, y_train_co, y_test_co = train_test_split(
    X_co_sampled, y_co_sampled, test_size=0.2, shuffle=False
)

# Predict
pred_oz = model_ozone.predict(X_test_oz)
pred_co = model_co.predict(X_test_co)

scaler_ozone = MinMaxScaler()
scaler_ozone.fit(y_ozone.reshape(-1, 1))

pred_ozone_original = scaler_ozone.inverse_transform(pred_oz)

# Pad predictions to match scaler input shape (e.g., (2110, 2))
import numpy as np

pred_ozone_padded = np.hstack((pred_oz, np.zeros_like(pred_oz)))
pred_ozone_inverse = scaler_ozone.inverse_transform(pred_ozone_padded)[:, 0]  # Take only the first column

print("Scaler input shape:", scaler_ozone.n_features_in_)
print("Prediction shape:", pred_oz.shape)

import matplotlib.pyplot as plt

def plot_loss(history, title):
    plt.figure(figsize=(8, 4))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title(title)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

plot_loss(history_ozone, 'Ozone Model Loss')
plot_loss(history_co, 'CO Model Loss')

df_ozone.resample('D', on='Datetime')['Pollutant_Level'].mean().plot()

df_ozone.groupby(['Latitude', 'Longitude']).corr()

plt.figure(figsize=(12, 6))
plt.plot(df_ozone['Datetime'], df_ozone['Pollutant_Level'], label='Ozone')
plt.plot(df_co['Datetime'], df_co['Pollutant_Level'], label='CO')
plt.title('Pollutant Levels Over Time')
plt.xlabel('Datetime')
plt.ylabel('Concentration (ppm)')
plt.legend()
plt.grid(True)
plt.show()

predictions_ozone = model_ozone.predict(X_ozone_sampled[:10])  # Predict first 10 samples
predictions_co = model_co.predict(X_co_sampled[:10])

print("Predicted Ozone Levels:", predictions_ozone)
print("Predicted CO Levels:", predictions_co)

import numpy as np

# Reshape predictions to (10, 1) â†’ (10, 2) by adding a second dummy column
predictions_ozone_reshaped = np.hstack([predictions_ozone, np.zeros((10, 1))])
predictions_co_reshaped = np.hstack([predictions_co, np.zeros((10, 1))])

# Apply inverse transformation
y_pred_ozone_original = scaler_ozone.inverse_transform(predictions_ozone_reshaped)[:, 0]  # Take only the first column
y_pred_co_original = scaler_co.inverse_transform(predictions_co_reshaped)[:, 0]

print("Original Scale Ozone Predictions:", y_ozone)
print("Original Scale CO Predictions:", y_co)

print("Shape of y_pred_ozone:", predictions_ozone.shape)
print("Shape of y_pred_co:", predictions_co.shape)
print("Shape of y_ozone_original:", y_ozone.shape)
print("Shape of y_co_original:", y_co.shape)

print(f"Scaler min shape: {scaler_ozone.min_.shape}")  # Should match y_pred_ozone
print(f"Scaler scale shape: {scaler_ozone.scale_.shape}")

print(f"y_pred_ozone shape: {predictions_ozone.shape}")  # Check if it is (N, 1) or (N, 2)
print(f"y_ozone_sampled shape: {y_ozone_sampled.shape}")

scaler_ozone = MinMaxScaler()
scaler_ozone.fit(y_ozone_sampled[:, 0].reshape(-1, 1))  # Fit only the first column

y_pred_ozone_original = scaler_ozone.inverse_transform(predictions_ozone)
y_ozone_original = scaler_ozone.inverse_transform(y_ozone_sampled)

y_pred_ozone_original = scaler_ozone.inverse_transform(predictions_ozone.reshape(-1, 1))
y_ozone_original = scaler_ozone.inverse_transform(y_ozone_sampled.reshape(-1, 1))

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

print(f"CO Target Variance: {np.var(y_co)}")

# Select only numeric columns before correlation
df_co_numeric = df_co.select_dtypes(include=['number'])

# Compute correlation matrix
print(df_co_numeric.corr())

print(df_co.dtypes)

df_co = df_co.apply(pd.to_numeric, errors='coerce')  # Convert non-numeric to NaN
df_co = df_co.dropna(axis=1, how='all')  # Drop columns that are completely NaN

print(df_co.corr())

import matplotlib.pyplot as plt
import seaborn as sns

# Configure plot style
sns.set(style="whitegrid", palette="muted")
plt.rcParams.update({'font.size': 14, 'figure.dpi': 150})

# Adjust figure size to prevent compression
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12), gridspec_kw={'height_ratios': [1, 1]})

# Ozone Prediction Plot
ax1.plot(y_test_oz[:10000], label='Actual Ozone', color='#1f77b4', linewidth=2)
ax1.plot(pred_oz[:10000], '--', label='Predicted Ozone', color='#ff7f0e', linewidth=2)
ax1.set_ylabel('Ozone Level', fontsize=14)
ax1.set_title('Ozone Prediction Over Time', fontsize=16)
ax1.legend(fontsize=12)
ax1.grid(True)

# CO Prediction Plot
ax2.plot(y_test_co[:10000], label='Actual CO', color='red', linewidth=2)
ax2.plot(pred_co[:10000], '--', label='Predicted CO', color='blue', linewidth=2)
ax2.set_xlabel('Time Steps', fontsize=14)
ax2.set_ylabel('CO Level', fontsize=14)
ax2.set_title('CO Prediction Over Time', fontsize=16)
ax2.legend(fontsize=12)
ax2.grid(True)

# Adjust layout to prevent overlap
plt.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Compute correlation matrix for CO dataset
df_co_numeric = df_co.select_dtypes(include=['number'])  # Select only numeric columns
corr_matrix_co = df_co_numeric.corr()

# Plot heatmap for CO dataset
plt.figure(figsize=(4, 3))  # Adjust figure size for better visibility
sns.heatmap(corr_matrix_co, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Correlation Heatmap - CO Dataset", fontsize=8)
plt.show()

# Compute correlation matrix for Ozone dataset
df_ozone_numeric = df_ozone.select_dtypes(include=['number'])  # Select only numeric columns
corr_matrix_ozone = df_ozone_numeric.corr()

# Plot heatmap for Ozone dataset
plt.figure(figsize=(4, 3))  # Adjust figure size for better visibility
sns.heatmap(corr_matrix_ozone, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Correlation Heatmap - Ozone Dataset", fontsize=8)
plt.show()

from sklearn.metrics import mean_squared_error, mean_absolute_error

# Assuming y_test_oz, pred_oz, y_test_co, and pred_co are defined in the notebook
# Replace these with actual test data and predictions
y_test_oz = np.random.rand(100)  # Replace with actual test data for Ozone
pred_oz = np.random.rand(100)    # Replace with actual predictions for Ozone

y_test_co = np.random.rand(100)  # Replace with actual test data for CO
pred_co = np.random.rand(100)    # Replace with actual predictions for CO

# Calculate MSE and MAE for Ozone predictions
mse_oz = mean_squared_error(y_test_oz, pred_oz)
mae_oz = mean_absolute_error(y_test_oz, pred_oz)

# Calculate MSE and MAE for CO predictions
mse_co = mean_squared_error(y_test_co, pred_co)
mae_co = mean_absolute_error(y_test_co, pred_co)

print(f"Ozone MSE: {mse_oz:.4f}, Ozone MAE: {mae_oz:.4f}")
print(f"CO MSE: {mse_co:.4f}, CO MAE: {mae_co:.4f}")

from sklearn.metrics import confusion_matrix
import numpy as np

# Example binary classification data for confusion matrix
# Replace these with actual binary classification labels and predictions
y_test_oz_binary = np.random.randint(0, 2, 100)  # Replace with actual binary test labels for Ozone
pred_oz_binary = np.random.randint(0, 2, 100)   # Replace with actual binary predictions for Ozone

y_test_co_binary = np.random.randint(0, 2, 100) # Replace with actual binary test labels for CO
pred_co_binary = np.random.randint(0, 2, 100)   # Replace with actual binary predictions for CO

# Confusion matrix for Ozone
conf_matrix_oz = confusion_matrix(y_test_oz_binary, pred_oz_binary)

# Confusion matrix for CO
conf_matrix_co = confusion_matrix(y_test_co_binary, pred_co_binary)

print("Confusion Matrix - Ozone:")
print(conf_matrix_oz)

print("\nConfusion Matrix - CO:")
print(conf_matrix_co)

# Hyperparameter Optimization:
!pip install keras-tuner --upgrade

import keras_tuner as kt

def model_builder(hp):
    model = tf.keras.Sequential()

    # Tune number of LSTM layers (1-3)
    for i in range(hp.Int('num_layers', 1, 3)):
        model.add(tf.keras.layers.LSTM(
            units=hp.Int(f'units_{i}', min_value=32, max_value=256, step=32),
            return_sequences=i < hp.get('num_layers')-1
        ))
        model.add(tf.keras.layers.Dropout(
            hp.Float(f'dropout_{i}', 0.1, 0.5, step=0.1)
        ))

    # Final Dense layers
    model.add(tf.keras.layers.Dense(
        hp.Int('dense_units', 16, 64, step=16),
        activation='relu'
    ))
    model.add(tf.keras.layers.Dense(1))

    # Tune learning rate
    hp_learning_rate = hp.Float('lr', 1e-4, 1e-2, sampling='log')

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),
        loss='mse',
        metrics=['mae']
    )
    return model

tuner = kt.Hyperband(
    model_builder,
    objective='val_mae',
    max_epochs=50,
    factor=3,
    directory='keras_tuner',
    project_name='air_quality'
)

stop_early = tf.keras.callbacks.EarlyStopping(
    monitor='val_mae',
    patience=10,
    restore_best_weights=True
)

import keras_tuner as kt
import tensorflow as tf

# 1. Streamlined Model Builder
def model_builder(hp):
    model = tf.keras.Sequential()

    # Reduced search space
    num_layers = hp.Int('num_layers', 1, 2)  # Was 1-3
    units = hp.Int('units', 32, 128, step=32)  # Was 32-256

    for _ in range(num_layers):
        model.add(tf.keras.layers.LSTM(
            units=units,
            return_sequences=_ < num_layers-1
        ))
        model.add(tf.keras.layers.Dropout(
            hp.Float('dropout', 0.2, 0.4, step=0.1)  # Narrowed range
        ))

    model.add(tf.keras.layers.Dense(1))

    # Constrained learning rate
    lr = hp.Float('lr', 1e-4, 1e-3, sampling='log')  # Reduced upper bound

    model.compile(
        optimizer=tf.keras.optimizers.Adam(lr),
        loss='mse',
        metrics=['mae']
    )
    return model

# 2. Faster Tuner Configuration
tuner = kt.RandomSearch(  # Switched from Hyperband to RandomSearch
    model_builder,
    objective='val_mae',
    max_trials=20,  # Reduced from 50+
    executions_per_trial=1,
    directory='keras_tuner',
    project_name='air_quality_fast'
)

# 3. Early Stopping with Aggressive Patience
stop_early = tf.keras.callbacks.EarlyStopping(
    monitor='val_mae',
    patience=3,  # Reduced from 10
    min_delta=0.001,
    restore_best_weights=True
)

# 4. Progress Monitoring
class TimeReporter(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        print(f"Epoch {epoch+1} - val_mae: {logs['val_mae']:.4f}")

# 5. Execute with Reduced Data Sample
tuner.search(
    X_ozone_sampled[:5000], y_ozone_sampled[:5000],  # Use subset of data
    validation_split=0.2,
    batch_size=128,  # Increased batch size
    epochs=30,  # Reduced from 50
    callbacks=[stop_early, TimeReporter()],
    verbose=1
)

best_hps_ozone = tuner.get_best_hyperparameters(num_trials=1)[0]
best_hps_co = tuner.get_best_hyperparameters(num_trials=1)[0]  # Repeat for CO

print(f"""
Optimal parameters for Ozone:
- Layers: {best_hps_ozone.get('num_layers')}
- Units: {best_hps_ozone.get('units')}  # Get the single 'units' value
- Dropout: {best_hps_ozone.get('dropout')}  # Get the single 'dropout' value
- Learning rate: {best_hps_ozone.get('lr')}
""")
print(f"""
Optimal parameters for CO:
- Layers: {best_hps_co.get('num_layers')}
- Units: {best_hps_co.get('units')}  # Get the single 'units' value
- Dropout: {best_hps_co.get('dropout')}  # Get the single 'dropout' value
- Learning rate: {best_hps_co.get('lr')}
""")

# Build ozone model
model_ozone = tuner.hypermodel.build(best_hps_ozone)

# Build CO model
model_co = tuner.hypermodel.build(best_hps_co)

# For Ozone
history_ozone = model_ozone.fit(
    X_ozone_sampled, y_ozone_sampled,
    epochs=200,
    validation_split=0.2,
    batch_size=128,  # Set a fixed batch size or use a value from your tuner search
    # callbacks=[stop_early],
    verbose=1
)
# For CO
history_co = model_co.fit(
    X_co_sampled, y_co_sampled,
    epochs=200,
    validation_split=0.2,
    batch_size=128,  # Set a fixed batch size or use best_hps_co.get('batch_size') if using tuned value
    # callbacks=[stop_early],
    verbose=1
)

# Repeat for CO with its best hyperparameters and appropriate batch size

tuner = kt.BayesianOptimization(
    model_builder,
    objective='val_mae',
    max_trials=50,
    num_initial_points=10,
    directory='keras_tuner',
    project_name='air_quality_final'
)

pip install optuna

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from scipy import stats

# In the preprocessing section, before applying scaling:

# For ozone
scaler_ozone = MinMaxScaler()
df_ozone['Pollutant_Level'] = scaler_ozone.fit_transform(df_ozone[['Pollutant_Level']]) #Fit only on Pollutant_Level

# For CO
scaler_co = MinMaxScaler()
df_co['Pollutant_Level'] = scaler_co.fit_transform(df_co[['Pollutant_Level']]) #Fit only on Pollutant_Level

def plot_error_distributions(y_true, pred, scaler, pollutant_name):
    # Inverse transform predictions and actual values
    y_true_orig = scaler.inverse_transform(y_true.reshape(-1, 1)) #y_true has the correct shape (n_samples, 1)
    pred_orig = scaler.inverse_transform(pred.reshape(-1, 1)) #pred has the correct shape (n_samples, 1)


    # Calculate errors
    errors = pred_orig - y_true_orig
    mae = np.mean(np.abs(errors))
    rmse = np.sqrt(np.mean(errors**2))

    plt.figure(figsize=(15, 10))

    # Histogram with KDE
    plt.subplot(2, 2, 1)
    sns.histplot(errors, kde=True, color='skyblue')
    plt.title(f'{pollutant_name} Error Distribution\nMAE: {mae:.3f} ppm, RMSE: {rmse:.3f} ppm')
    plt.xlabel('Prediction Error (ppm)')
    plt.axvline(x=0, color='r', linestyle='--')
    plt.annotate(f'Mean: {errors.mean():.3f}\nStd: {errors.std():.3f}',
                 xy=(0.7, 0.85), xycoords='axes fraction')

    # Q-Q Plot
    plt.subplot(2, 2, 2)
    stats.probplot(errors.flatten(), plot=plt)
    plt.title('Q-Q Plot of Prediction Errors')
    plt.grid(True)

    # Error vs Actual Values
    plt.subplot(2, 2, 3)
    plt.scatter(y_true_orig, errors, alpha=0.5)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('Actual Values (ppm)')
    plt.ylabel('Prediction Error (ppm)')
    plt.title('Error vs Actual Concentration')

    # Boxplot
    plt.subplot(2, 2, 4)
    sns.boxplot(x=errors.flatten(), color='lightgreen')
    plt.title('Error Distribution Boxplot')
    plt.xlabel('Prediction Error (ppm)')

    plt.tight_layout()
    plt.savefig(f'{pollutant_name}_error_distribution.pdf')
    plt.show()

# For Ozone
plot_error_distributions(y_test_oz, pred_oz, scaler_ozone, 'Ozone')

# For CO
plot_error_distributions(y_test_co, pred_co, scaler_co, 'Carbon_Monoxide')

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from scipy import stats

# Function to plot and save error distributions
def save_error_distributions(y_true, pred, scaler, pollutant_name):
    # Inverse transform predictions and actual values
    y_true_orig = scaler.inverse_transform(y_true.reshape(-1, 1))
    pred_orig = scaler.inverse_transform(pred.reshape(-1, 1))

    # Calculate errors
    errors = pred_orig - y_true_orig
    mae = np.mean(np.abs(errors))
    rmse = np.sqrt(np.mean(errors**2))

    plt.figure(figsize=(15, 10))

    # Histogram with KDE
    plt.subplot(2, 2, 1)
    sns.histplot(errors, kde=True, color='skyblue')
    plt.title(f'{pollutant_name} Error Distribution\nMAE: {mae:.3f} ppm, RMSE: {rmse:.3f} ppm')
    plt.xlabel('Prediction Error (ppm)')
    plt.axvline(x=0, color='r', linestyle='--')
    plt.annotate(f'Mean: {errors.mean():.3f}\nStd: {errors.std():.3f}',
                 xy=(0.7, 0.85), xycoords='axes fraction')

    # Q-Q Plot
    plt.subplot(2, 2, 2)
    stats.probplot(errors.flatten(), plot=plt)
    plt.title('Q-Q Plot of Prediction Errors')
    plt.grid(True)

    # Error vs Actual Values
    plt.subplot(2, 2, 3)
    plt.scatter(y_true_orig, errors, alpha=0.5)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('Actual Values (ppm)')
    plt.ylabel('Prediction Error (ppm)')
    plt.title('Error vs Actual Concentration')

    # Boxplot
    plt.subplot(2, 2, 4)
    sns.boxplot(x=errors.flatten(), color='lightgreen')
    plt.title('Error Distribution Boxplot')
    plt.xlabel('Prediction Error (ppm)')

    # Save the plot as an image file
    plt.tight_layout()
    plt.savefig(f'{pollutant_name}_error_distribution.png', dpi=300)
    plt.close()

# Example usage:
# Ensure these variables are defined before using the function:
# y_test_oz, pred_oz, scaler_ozone for Ozone
# y_test_co, pred_co, scaler_co for Carbon Monoxide

save_error_distributions(y_test_oz, pred_oz, scaler_ozone, 'Ozone')
save_error_distributions(y_test_co, pred_co, scaler_co, 'Carbon_Monoxide')















"""Post Training"""